{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Tensor\n",
    "x = tf.ones((2, 2))\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  4.0\n",
      "z:  16.0\n"
     ]
    }
   ],
   "source": [
    "# Using GradientTape API for AutoDiff\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    print(\"y: \", y.numpy())\n",
    "    z = tf.multiply(y, y)\n",
    "    print(\"z: \", z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  [[8. 8.]\n",
      " [8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "# Derivative of z with respect to the original input tensor x\n",
    "# z = y^2\n",
    "# y = summation(x)\n",
    "# dz/dx = d(y^2)/dx = 2y dy/dx = 2(summation(x)) * dx/dx = 2 * (summation(x)) = 2 * 4 = 8.0\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(\"dz/dx: \", dz_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Gradient values are correct\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate Gradient Computation\n",
    "\n",
    "Gradients of the output with respect to intermediate values computed during a \"recorded\" tf.GradientTape context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  4.0\n",
      "z:  16.0\n"
     ]
    }
   ],
   "source": [
    "# Using GradientTape API for AutoDiff\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    print(\"y: \", y.numpy())\n",
    "    z = tf.multiply(y, y)\n",
    "    print(\"z: \", z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dy:  8.0\n"
     ]
    }
   ],
   "source": [
    "# Derivative of z with respect to the intermediate tensor y\n",
    "# z = y^2\n",
    "# y = summation(x)\n",
    "# dz/dy = d(y^2)/dy = 2y * dy/dy = 2y = 2 * 4.0 = 8.0\n",
    "dz_dy = t.gradient(z,y)\n",
    "print(\"dz/dy: \", dz_dy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistant Gradients\n",
    "\n",
    "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Tensor\n",
    "x = tf.constant(3.0)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  9.0\n",
      "z:  81.0\n"
     ]
    }
   ],
   "source": [
    "# Using GradientTape API for AutoDiff with Persistant Gradients\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x * x\n",
    "    print(\"y: \", y.numpy())\n",
    "    z = y * y\n",
    "    print(\"z: \", z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  108.0\n"
     ]
    }
   ],
   "source": [
    "# Derivative of z with respect to the original input tensor x\n",
    "# z = y^2\n",
    "# y = x^2\n",
    "# dz/dx = d(y^2)/dx = 2y * dy/dx = 2 * x^2 * d(x^2)/dx = 2 * x^2 * 2x * dx/dx = 4 * x^3 = 108.0\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(\"dz/dx: \", dz_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx:  6.0\n"
     ]
    }
   ],
   "source": [
    "# Derivative of y with respect to the input tensor y\n",
    "# dy/dx = d(x^2)/dx = 2*x = 6.0\n",
    "dy_dx = t.gradient(y, x)\n",
    "print(\"dy/dx: \", dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the reference to the tape\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording Control Flow\n",
    "\n",
    "Because tapes record operations as they are executed, Python control flow (using ifs and whiles for example) is naturally handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Function\n",
    "def sample(x, y):\n",
    "    output = 1.0\n",
    "    for i in range(y):\n",
    "        if i > 1 and i < 5:\n",
    "            output = tf.multiply(output, x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Grad Function\n",
    "def grad(x, y):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        # get output from function\n",
    "        out = sample(x, y)\n",
    "        print(\"output: \", out)\n",
    "    # Return gradient of output w.r.t input\n",
    "    return t.gradient(out, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Input Tensor\n",
    "x = tf.convert_to_tensor(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "output:  tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "output:  tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "assert grad(x, 6).numpy() == 12.0\n",
    "assert grad(x, 5).numpy() == 12.0\n",
    "assert grad(x, 4).numpy() == 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-order gradients\n",
    "\n",
    "Operations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Input Tensor\n",
    "x = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  1.0\n",
      "dy/dx:  3.0\n",
      "d2y/dx2:  6.0\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = x * x * x\n",
    "        print('y: ', y.numpy())\n",
    "        \n",
    "    # Compute Gradient of y w.r.t input x\n",
    "    # y = x^3\n",
    "    # dy/dx = 3 * x^2 = 3.0\n",
    "    dy_dx = t2.gradient(y, x)\n",
    "    print(\"dy/dx: \", dy_dx.numpy())\n",
    "    \n",
    "# Compute Gradient of Gradient of y w.r.t x w.r.t x\n",
    "# dy/dx = 3 * x^2\n",
    "# d2y/dx2 = 3 * 2x = 6x = 6.0\n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "print(\"d2y/dx2: \", d2y_dx2.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
